{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM - no context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 23:19:40.869932: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-26 23:19:41.033556: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-26 23:19:41.780159: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-26 23:19:41.783129: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 23:19:45.291207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Required modules\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user:token</th>\n",
       "      <th>item:token</th>\n",
       "      <th>cnt:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user:token  item:token  cnt:float\n",
       "0           0           0          0\n",
       "1           1           1          1\n",
       "2           2           2          1\n",
       "3           3           3          0\n",
       "4           4           4          3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "use_cols = ['user:token', 'item:token', 'cnt:float']\n",
    "data = pd.read_csv('./dataset/frappe/frappe.inter', usecols=use_cols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the user_id column\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "data['user:token'] = user_encoder.fit_transform(data['user:token'].values)\n",
    "\n",
    "# Encoding the item_id column\n",
    "item_encoder = LabelEncoder()\n",
    "data['item:token'] = item_encoder.fit_transform(data['item:token'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "\n",
    "data.columns = ['user_id', 'item_id', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of unique users and movies\n",
    "\n",
    "num_users = data['user_id'].nunique()\n",
    "num_movies = data['item_id'].nunique()\n",
    "\n",
    "# Define embedding size\n",
    "\n",
    "embedding_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "def get_model(embedding_size, weight_decay=0.0):\n",
    "    # Define the input shape\n",
    "    input_shape = (train_data.shape[1] - 1,)\n",
    "\n",
    "    l2_reg = tf.keras.regularizers.l2(weight_decay)\n",
    "\n",
    "    # Define input layers\n",
    "    user_input = tf.keras.layers.Input(shape=(1,))\n",
    "    movie_input = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "    # Define user embedding\n",
    "    user_embedding = tf.keras.layers.Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "    user_embedding = tf.keras.layers.Flatten()(user_embedding)\n",
    "\n",
    "    # Define movie embedding\n",
    "    movie_embedding = tf.keras.layers.Embedding(num_movies, embedding_size, input_length=1)(movie_input)\n",
    "    movie_embedding = tf.keras.layers.Flatten()(movie_embedding)\n",
    "\n",
    "    # Concatenate user and movie embeddings\n",
    "    concat = tf.keras.layers.concatenate([user_embedding, movie_embedding])\n",
    "\n",
    "    # Define FM part\n",
    "    fm = tf.keras.layers.Dense(1, activation=None)(concat)\n",
    "\n",
    "    # Define DNN part\n",
    "    dnn = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2_reg)(concat)\n",
    "    dnn = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=l2_reg)(dnn)\n",
    "    dnn = tf.keras.layers.Dense(1, activation=None)(dnn)\n",
    "\n",
    "    # Concatenate FM and DNN parts\n",
    "    concat = tf.keras.layers.concatenate([fm, dnn])\n",
    "\n",
    "    # Define output layer\n",
    "    output = tf.keras.layers.Flatten()(concat)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[user_input, movie_input], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1203/1203 [==============================] - 3s 2ms/step - loss: 1.1197 - mae: 0.8103 - val_loss: 0.8664 - val_mae: 0.7309\n",
      "Epoch 2/100\n",
      "1203/1203 [==============================] - 2s 2ms/step - loss: 0.7605 - mae: 0.6843 - val_loss: 0.6888 - val_mae: 0.6507\n",
      "Epoch 3/100\n",
      "1203/1203 [==============================] - 2s 2ms/step - loss: 0.6523 - mae: 0.6358 - val_loss: 0.6378 - val_mae: 0.6312\n",
      "Epoch 4/100\n",
      "1203/1203 [==============================] - 2s 2ms/step - loss: 0.6153 - mae: 0.6206 - val_loss: 0.6185 - val_mae: 0.6225\n",
      "Epoch 5/100\n",
      "1203/1203 [==============================] - 2s 2ms/step - loss: 0.5974 - mae: 0.6118 - val_loss: 0.6086 - val_mae: 0.6190\n",
      "Epoch 6/100\n",
      "1203/1203 [==============================] - 2s 2ms/step - loss: 0.5872 - mae: 0.6065 - val_loss: 0.6033 - val_mae: 0.6159\n",
      "Epoch 7/100\n",
      " 265/1203 [=====>........................] - ETA: 1s - loss: 0.5856 - mae: 0.6051"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit([train_data['user_id'], train_data['item_id']], \n",
    "                    train_data['rating'], \n",
    "                    validation_data=([test_data['user_id'], test_data['item_id']], test_data['rating']),\n",
    "                    epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.evaluate([test_data['user_id'], test_data['item_id']], test_data['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 655us/step - loss: 2.0597 - mae: 1.0770\n",
      "[2.0596837997436523, 1.0770288705825806]\n",
      "89/89 [==============================] - 0s 697us/step - loss: 2.0669 - mae: 1.0940\n",
      "[2.066863536834717, 1.0939524173736572]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m get_model(embedding_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mfit([train_set[\u001b[39m'\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m'\u001b[39;49m], train_set[\u001b[39m'\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m'\u001b[39;49m]], \n\u001b[1;32m     15\u001b[0m           train_set[\u001b[39m'\u001b[39;49m\u001b[39mrating\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     16\u001b[0m           validation_data\u001b[39m=\u001b[39;49m([valid_set[\u001b[39m'\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m'\u001b[39;49m], valid_set[\u001b[39m'\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m'\u001b[39;49m]], valid_set[\u001b[39m'\u001b[39;49m\u001b[39mrating\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     17\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mevaluate([test_data[\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m], test_data[\u001b[39m'\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m'\u001b[39m]], test_data[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:142\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m--> 142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:342\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_define_function\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs):\n\u001b[1;32m    322\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Gets a function for these inputs, defining it if necessary.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \n\u001b[1;32m    324\u001b[0m \u001b[39m  Caller must hold self._lock.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m      shape relaxation retracing.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m   args, kwargs, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 342\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_spec\u001b[39m.\u001b[39;49mcanonicalize_function_inputs(args, kwargs))\n\u001b[1;32m    344\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature, \u001b[39m*\u001b[39margs[\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature):])\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:408\u001b[0m, in \u001b[0;36mFunctionSpec.canonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m   args, kwargs \u001b[39m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[1;32m    407\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbind_function_inputs(args, kwargs)\n\u001b[0;32m--> 408\u001b[0m args, kwargs \u001b[39m=\u001b[39m cast_inputs(args, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature)\n\u001b[1;32m    409\u001b[0m filtered_flat_args \u001b[39m=\u001b[39m filter_function_inputs(args, kwargs)\n\u001b[1;32m    411\u001b[0m \u001b[39mreturn\u001b[39;00m args, kwargs, filtered_flat_args\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:480\u001b[0m, in \u001b[0;36mcast_inputs\u001b[0;34m(args, kwargs, input_signature)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Casts args, kwargs to TF values based on an optional input_signature.\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m input_signature \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m   args \u001b[39m=\u001b[39m cast_numpy_inputs(args)\n\u001b[1;32m    481\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m   args \u001b[39m=\u001b[39m cast_inputs_to_signature(args, input_signature)\n",
      "File \u001b[0;32m/scratch/apeddi/cs688-project/project/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:501\u001b[0m, in \u001b[0;36mcast_numpy_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mfor\u001b[39;00m index, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(flat_inputs):\n\u001b[1;32m    499\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value,\n\u001b[1;32m    500\u001b[0m                 (ops\u001b[39m.\u001b[39mTensor, resource_variable_ops\u001b[39m.\u001b[39mBaseResourceVariable)):\n\u001b[0;32m--> 501\u001b[0m     filtered_flat_inputs\u001b[39m.\u001b[39;49mappend(value)\n\u001b[1;32m    502\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39m__array__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m    503\u001b[0m       \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39m_should_act_as_resource_variable\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    504\u001b[0m       \u001b[39misinstance\u001b[39m(value, (np\u001b[39m.\u001b[39mstr_, \u001b[39mtype\u001b[39m, composite_tensor\u001b[39m.\u001b[39mCompositeTensor))):\n\u001b[1;32m    505\u001b[0m     \u001b[39m# This case is equivalent to _is_ndarray(value) == True\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     a \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39m__array__()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, valid_index in kf.split(train_data):\n",
    "    train_set = train_data.iloc[train_index]\n",
    "    valid_set = train_data.iloc[valid_index]\n",
    "    \n",
    "    model = get_model(embedding_size=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    model.fit([train_set['user_id'], train_set['item_id']], \n",
    "              train_set['rating'], \n",
    "              validation_data=([valid_set['user_id'], valid_set['item_id']], valid_set['rating']),\n",
    "              epochs=100, batch_size=16, verbose=0)\n",
    "    print(model.evaluate([test_data['user_id'], test_data['item_id']], test_data['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 1s 981us/step - loss: 0.5919 - mae: 0.6037\n",
      "602/602 [==============================] - 1s 955us/step - loss: 0.5688 - mae: 0.5872\n",
      "602/602 [==============================] - 1s 908us/step - loss: 0.5828 - mae: 0.5964\n",
      "602/602 [==============================] - 1s 955us/step - loss: 0.5742 - mae: 0.5924\n",
      "602/602 [==============================] - 1s 911us/step - loss: 0.5912 - mae: 0.6018\n",
      "602/602 [==============================] - 1s 957us/step - loss: 0.5917 - mae: 0.6027\n",
      "602/602 [==============================] - 1s 966us/step - loss: 0.5691 - mae: 0.5884\n",
      "602/602 [==============================] - 1s 975us/step - loss: 0.5858 - mae: 0.5988\n",
      "602/602 [==============================] - 1s 963us/step - loss: 0.5768 - mae: 0.5923\n",
      "602/602 [==============================] - 1s 960us/step - loss: 0.5935 - mae: 0.6008\n",
      "602/602 [==============================] - 1s 959us/step - loss: 0.5897 - mae: 0.6018\n",
      "602/602 [==============================] - 1s 980us/step - loss: 0.5692 - mae: 0.5885\n",
      "602/602 [==============================] - 1s 918us/step - loss: 0.5869 - mae: 0.5999\n",
      "602/602 [==============================] - 1s 970us/step - loss: 0.5743 - mae: 0.5910\n",
      "602/602 [==============================] - 1s 975us/step - loss: 0.5914 - mae: 0.5996\n",
      "602/602 [==============================] - 1s 960us/step - loss: 0.5909 - mae: 0.6021\n",
      "602/602 [==============================] - 1s 934us/step - loss: 0.5694 - mae: 0.5883\n",
      "602/602 [==============================] - 1s 922us/step - loss: 0.5852 - mae: 0.5993\n",
      "602/602 [==============================] - 1s 969us/step - loss: 0.5747 - mae: 0.5916\n",
      "602/602 [==============================] - 1s 970us/step - loss: 0.5902 - mae: 0.5992\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5911 - mae: 0.6006\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5711 - mae: 0.5886\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5817 - mae: 0.5956\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5730 - mae: 0.5900\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5918 - mae: 0.5996\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5910 - mae: 0.6013\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5673 - mae: 0.5861\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5807 - mae: 0.5966\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5753 - mae: 0.5903\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5894 - mae: 0.5980\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5908 - mae: 0.6023\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5731 - mae: 0.5897\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5817 - mae: 0.5952\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5765 - mae: 0.5916\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5900 - mae: 0.5980\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5933 - mae: 0.6038\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5679 - mae: 0.5857\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5825 - mae: 0.5964\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5754 - mae: 0.5904\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5920 - mae: 0.5997\n",
      "602/602 [==============================] - 1s 962us/step - loss: 0.5890 - mae: 0.6009\n",
      "602/602 [==============================] - 1s 929us/step - loss: 0.5671 - mae: 0.5907\n",
      "602/602 [==============================] - 1s 975us/step - loss: 0.5811 - mae: 0.5976\n",
      "602/602 [==============================] - 1s 964us/step - loss: 0.5714 - mae: 0.5917\n",
      "602/602 [==============================] - 1s 950us/step - loss: 0.5846 - mae: 0.5967\n",
      "602/602 [==============================] - 1s 954us/step - loss: 0.5893 - mae: 0.6031\n",
      "602/602 [==============================] - 1s 946us/step - loss: 0.5686 - mae: 0.5911\n",
      "602/602 [==============================] - 1s 979us/step - loss: 0.5817 - mae: 0.6005\n",
      "602/602 [==============================] - 1s 976us/step - loss: 0.5717 - mae: 0.5919\n",
      "602/602 [==============================] - 1s 926us/step - loss: 0.5887 - mae: 0.6018\n",
      "602/602 [==============================] - 1s 899us/step - loss: 0.5894 - mae: 0.6014\n",
      "602/602 [==============================] - 1s 947us/step - loss: 0.5686 - mae: 0.5917\n",
      "602/602 [==============================] - 1s 951us/step - loss: 0.5813 - mae: 0.5986\n",
      "602/602 [==============================] - 1s 963us/step - loss: 0.5724 - mae: 0.5906\n",
      "602/602 [==============================] - 1s 903us/step - loss: 0.5869 - mae: 0.6001\n",
      "602/602 [==============================] - 1s 949us/step - loss: 0.5885 - mae: 0.6050\n",
      "602/602 [==============================] - 1s 955us/step - loss: 0.5683 - mae: 0.5905\n",
      "602/602 [==============================] - 1s 925us/step - loss: 0.5840 - mae: 0.6007\n",
      "602/602 [==============================] - 1s 932us/step - loss: 0.5740 - mae: 0.5927\n",
      "602/602 [==============================] - 1s 958us/step - loss: 0.5916 - mae: 0.6051\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5862 - mae: 0.6016\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5639 - mae: 0.5867\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5784 - mae: 0.5957\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5691 - mae: 0.5926\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5863 - mae: 0.5978\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5860 - mae: 0.6012\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5632 - mae: 0.5886\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5780 - mae: 0.5978\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5689 - mae: 0.5910\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5855 - mae: 0.6000\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5881 - mae: 0.6005\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5651 - mae: 0.5888\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5795 - mae: 0.5964\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5718 - mae: 0.5931\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5865 - mae: 0.6009\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5881 - mae: 0.6031\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5640 - mae: 0.5887\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5806 - mae: 0.5998\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5741 - mae: 0.5947\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5844 - mae: 0.5993\n",
      "602/602 [==============================] - 1s 900us/step - loss: 0.5955 - mae: 0.6039\n",
      "602/602 [==============================] - 1s 962us/step - loss: 0.5798 - mae: 0.5959\n",
      "602/602 [==============================] - 1s 965us/step - loss: 0.5854 - mae: 0.6005\n",
      "602/602 [==============================] - 1s 950us/step - loss: 0.5861 - mae: 0.5976\n",
      "602/602 [==============================] - 1s 937us/step - loss: 0.5901 - mae: 0.5980\n",
      "602/602 [==============================] - 1s 972us/step - loss: 0.5957 - mae: 0.6034\n",
      "602/602 [==============================] - 1s 906us/step - loss: 0.5722 - mae: 0.5917\n",
      "602/602 [==============================] - 1s 894us/step - loss: 0.5852 - mae: 0.5993\n",
      "602/602 [==============================] - 1s 963us/step - loss: 0.5833 - mae: 0.5972\n",
      "602/602 [==============================] - 1s 956us/step - loss: 0.6008 - mae: 0.6058\n",
      "602/602 [==============================] - 1s 896us/step - loss: 0.5992 - mae: 0.6063\n",
      "602/602 [==============================] - 1s 959us/step - loss: 0.5697 - mae: 0.5877\n",
      "602/602 [==============================] - 1s 937us/step - loss: 0.5875 - mae: 0.6011\n",
      "602/602 [==============================] - 1s 940us/step - loss: 0.5771 - mae: 0.5947\n",
      "602/602 [==============================] - 1s 943us/step - loss: 0.5910 - mae: 0.6011\n",
      "602/602 [==============================] - 1s 932us/step - loss: 0.5979 - mae: 0.6064\n",
      "602/602 [==============================] - 1s 942us/step - loss: 0.5685 - mae: 0.5872\n",
      "602/602 [==============================] - 1s 955us/step - loss: 0.5930 - mae: 0.6012\n",
      "602/602 [==============================] - 1s 983us/step - loss: 0.5790 - mae: 0.5929\n",
      "602/602 [==============================] - 1s 971us/step - loss: 0.5870 - mae: 0.5966\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6104 - mae: 0.6162\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6055 - mae: 0.6111\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6010 - mae: 0.6104\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5914 - mae: 0.6026\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6088 - mae: 0.6079\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5948 - mae: 0.6046\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5737 - mae: 0.5941\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5926 - mae: 0.6026\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5818 - mae: 0.5951\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5986 - mae: 0.6068\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6103 - mae: 0.6139\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5841 - mae: 0.5999\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6104 - mae: 0.6147\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5915 - mae: 0.6030\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6145 - mae: 0.6136\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6017 - mae: 0.6060\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5863 - mae: 0.5971\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5951 - mae: 0.6059\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5912 - mae: 0.5981\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5959 - mae: 0.6028\n",
      "602/602 [==============================] - 1s 964us/step - loss: 0.5908 - mae: 0.6046\n",
      "602/602 [==============================] - 1s 898us/step - loss: 0.5677 - mae: 0.5911\n",
      "602/602 [==============================] - 1s 952us/step - loss: 0.5828 - mae: 0.5989\n",
      "602/602 [==============================] - 1s 966us/step - loss: 0.5739 - mae: 0.5962\n",
      "602/602 [==============================] - 1s 937us/step - loss: 0.5908 - mae: 0.6017\n",
      "602/602 [==============================] - 1s 954us/step - loss: 0.5897 - mae: 0.6029\n",
      "602/602 [==============================] - 1s 968us/step - loss: 0.5665 - mae: 0.5865\n",
      "602/602 [==============================] - 1s 930us/step - loss: 0.5824 - mae: 0.5991\n",
      "602/602 [==============================] - 1s 970us/step - loss: 0.5720 - mae: 0.5902\n",
      "602/602 [==============================] - 1s 985us/step - loss: 0.5935 - mae: 0.6029\n",
      "602/602 [==============================] - 1s 906us/step - loss: 0.5975 - mae: 0.6064\n",
      "602/602 [==============================] - 1s 895us/step - loss: 0.5685 - mae: 0.5866\n",
      "602/602 [==============================] - 1s 914us/step - loss: 0.5801 - mae: 0.5977\n",
      "602/602 [==============================] - 1s 895us/step - loss: 0.5777 - mae: 0.5942\n",
      "602/602 [==============================] - 1s 936us/step - loss: 0.5913 - mae: 0.5998\n",
      "602/602 [==============================] - 1s 948us/step - loss: 0.5935 - mae: 0.6037\n",
      "602/602 [==============================] - 1s 908us/step - loss: 0.5733 - mae: 0.5877\n",
      "602/602 [==============================] - 1s 950us/step - loss: 0.5838 - mae: 0.5990\n",
      "602/602 [==============================] - 1s 958us/step - loss: 0.5774 - mae: 0.5915\n",
      "602/602 [==============================] - 1s 894us/step - loss: 0.5944 - mae: 0.6036\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5969 - mae: 0.6060\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5808 - mae: 0.5911\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5887 - mae: 0.6023\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5769 - mae: 0.5947\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5894 - mae: 0.6035\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5974 - mae: 0.6052\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5850 - mae: 0.6033\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5870 - mae: 0.5978\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5741 - mae: 0.5923\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.6072 - mae: 0.6066\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5987 - mae: 0.6078\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5800 - mae: 0.5978\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5945 - mae: 0.6033\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5894 - mae: 0.6002\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5925 - mae: 0.6017\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5947 - mae: 0.6015\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5727 - mae: 0.5917\n",
      "602/602 [==============================] - 1s 1ms/step - loss: 0.5826 - mae: 0.5966\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "filename = 'frappe_hyper_deepfm_gs.csv'\n",
    "hyper = pd.read_csv(f\"./hypers/version2/{filename}\")\n",
    "learning_rate = [1e-6, 1e-5, 1e-3, 1e-1]\n",
    "learner = ['adam', 'RMSprop']\n",
    "epochs = [10, 20, 30]\n",
    "embedding_size = [32, 128, 512]\n",
    "weight_decay = [1e-3, 1e-1]\n",
    "train_batch_size = [500, 1000]\n",
    "\n",
    "for lr, learner, embedding_size, weight_decay, epoch, train_batch_size in product(learning_rate, learner, embedding_size, weight_decay, epochs, train_batch_size):\n",
    "    kf = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "    for train_index, valid_index in kf.split(data):\n",
    "        train_set = data.iloc[train_index]\n",
    "        valid_set = data.iloc[valid_index]\n",
    "        model = get_model(embedding_size=embedding_size)\n",
    "        if learner == 'adam':\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "        else:\n",
    "            model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    \n",
    "        model.fit([train_set['user_id'], train_set['item_id']], \n",
    "                  train_set['rating'], \n",
    "                  validation_data=([valid_set['user_id'], valid_set['item_id']], valid_set['rating']),\n",
    "                  epochs=epoch, batch_size=train_batch_size, verbose=0)\n",
    "        rmse, mae = model.evaluate([valid_set['user_id'], valid_set['item_id']], valid_set['rating'])\n",
    "        hyper = pd.concat([hyper, pd.DataFrame(\n",
    "            [[lr, learner, embedding_size, weight_decay, train_batch_size, mae, rmse]], columns=['learning_rate', 'learner', 'embedding_size', 'weight_decay', 'train_batch_size', 'epoch', 'mae', 'rmse']\n",
    "        )])\n",
    "        hyper.to_csv(f\"./hypers/version2/{filename}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
